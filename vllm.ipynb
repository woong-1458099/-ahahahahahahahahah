{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Objectives**\n",
                "\n",
                "1. ì‹¤ìŠµëª… : vLLMì„ ì´ìš©í•œ íš¨ìœ¨ì ì¸ LLM ì¶”ë¡ ê³¼ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹\n",
                "2. í•µì‹¬ ì£¼ì œ:\n",
                "    1. vLLMì˜ PagedAttentionê³¼ KV Cache ê°œë… ì´í•´\n",
                "    2. LoRA adapter í†µí•© ë°©ë²• (Runtime LoRA vs Merged Model)\n",
                "    3. Transformers vs vLLM ì„±ëŠ¥ ë¹„êµ (ì†ë„, ë©”ëª¨ë¦¬, ì²˜ë¦¬ëŸ‰)\n",
                "    4. Text-to-SQL taskë¥¼ ìœ„í•œ ì‹¤ì „ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "3. í•™ìŠµ ëª©í‘œ :\n",
                "    1. vLLMì˜ PagedAttention ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì˜ ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
                "    2. LoRA adapterë¥¼ vLLMì— í†µí•©í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n",
                "    3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•´ vLLMì˜ ì¥ì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆë‹¤.\n",
                "4. í•™ìŠµ ê°œë…: í‚¤ì›Œë“œëª… :\n",
                "    1. PagedAttention\n",
                "    2. KV Cache\n",
                "    3. vLLM\n",
                "    4. LoRA adapter\n",
                "    5. Performance Benchmarking\n",
                "5. í•™ìŠµ ë°©í–¥ :\n",
                "  - vLLMì˜ í•µì‹¬ ê¸°ìˆ ì¸ PagedAttentionì´ ì–´ë–»ê²Œ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ”ì§€ ì‹œë®¬ë ˆì´í„°ë¥¼ í†µí•´ ì²´í—˜í•©ë‹ˆë‹¤.\n",
                "  - ì‹¤ìŠµ ì½”ë“œëŠ” ë‹¨ê³„ë³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, Jupyter notebookì„ í†µí•´ ì´ë¡ ê³¼ ì‹¤ìŠµì„ ë³‘í–‰í•©ë‹ˆë‹¤.\n",
                "  - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ì„ í†µí•´ ì¼ë°˜ ì¶”ë¡  ë°©ì‹ê³¼ vLLMì˜ ì°¨ì´ë¥¼ ì§ì ‘ í™•ì¸í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Prerequisites**\n",
                "```\n",
                "numpy==2.1.0\n",
                "pandas==2.2.3\n",
                "transformers==4.56.0\n",
                "torch==2.9.0+cu129\n",
                "accelerate==1.10.1\n",
                "bitsandbytes==0.49.1\n",
                "datasets==4.0.0\n",
                "peft==0.17.1\n",
                "vllm==0.13.0\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1: vLLM ì•„í‚¤í…ì²˜ ë° í•µì‹¬ ì›ë¦¬\n",
                "        \n",
                "### 1.1 ë„ì… ë°°ê²½ (Why vLLM?)\n",
                "\n",
                "ê¸°ì¡´ HuggingFace Transformers ê¸°ë°˜ ì„œë¹™ ë°©ì‹ì€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë³‘ëª©(Bottleneck)ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
                "\n",
                "* **ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation)**: KV Cacheë¥¼ ì—°ì†ëœ ë©”ëª¨ë¦¬ ê³µê°„ì— í• ë‹¹í•´ì•¼ í•˜ë¯€ë¡œ, ì‹¤ì œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì˜ˆì•½ ë©”ëª¨ë¦¬(Internal Fragmentation)ë¡œ ì¸í•œ VRAM ë‚­ë¹„ê°€ ì‹¬ê°í•©ë‹ˆë‹¤.\n",
                "* **ë‚®ì€ ì²˜ë¦¬ëŸ‰(Throughput)**: Static Batching ë°©ì‹ì˜ ë¹„íš¨ìœ¨ì„±ìœ¼ë¡œ ì¸í•´ ë™ì‹œ ì ‘ì† ì²˜ë¦¬ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "**vLLMì˜ ì†”ë£¨ì…˜:**\n",
                "\n",
                "* **PagedAttention**: OSì˜ ê°€ìƒ ë©”ëª¨ë¦¬(Virtual Memory) ê¸°ë²•ì„ ì°¨ìš©í•˜ì—¬ KV Cacheë¥¼ ë¹„ì—°ì†ì  ë¸”ë¡ìœ¼ë¡œ ê´€ë¦¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
                "* **Continuous Batching**: ìš”ì²­ì´ ë“¤ì–´ì˜¤ëŠ” ì¦‰ì‹œ ë™ì ìœ¼ë¡œ ë°°ì¹˜ì— í¬í•¨ì‹œì¼œ GPU ê°€ë™ë¥ (Utilization)ì„ ë†’ì…ë‹ˆë‹¤.\n",
                "\n",
                "### 1.2 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 1: PagedAttention\n",
                "\n",
                "PagedAttentionì€ vLLM ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ìœ¼ë¡œ, ìš´ì˜ì²´ì œì˜ í˜ì´ì§•(Paging) ì‹œìŠ¤í…œê³¼ ë™ì¼í•œ ì›ë¦¬ì…ë‹ˆë‹¤.\n",
                "\n",
                "**Legacy ë°©ì‹ (Contiguous Allocation)**:\n",
                "\n",
                "* ìš”ì²­ ì‹œ ìµœëŒ€ í† í° ê¸¸ì´(Max Context Length)ë§Œí¼ì˜ ì—°ì†ëœ ë©”ëª¨ë¦¬ë¥¼ ë¯¸ë¦¬ í™•ë³´í•©ë‹ˆë‹¤.\n",
                "* ìƒì„±ë˜ì§€ ì•Šì€ ë¯¸ë˜ì˜ í† í°ì„ ìœ„í•œ ê³µê°„ê¹Œì§€ ì ìœ í•˜ë¯€ë¡œ **ë©”ëª¨ë¦¬ ë‚­ë¹„(Waste)**ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "\n",
                "```\n",
                "Physical Memory: [Token 1...Token N | Reserved (Unused) Space]\n",
                "-> ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë° OOM(Out of Memory)ì˜ ì£¼ì›ì¸\n",
                "\n",
                "```\n",
                "\n",
                "**PagedAttention ë°©ì‹ (Non-contiguous Allocation)**:\n",
                "\n",
                "* KV Cacheë¥¼ ê³ ì • í¬ê¸°ì˜ 'ë¸”ë¡(Block)' ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
                "* ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì§„ ë©”ëª¨ë¦¬ ê³µê°„ì´ë¼ë„ **Block Table**ì„ í†µí•´ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ì†ëœ ê²ƒì²˜ëŸ¼ ë§¤í•‘í•©ë‹ˆë‹¤.\n",
                "* í•„ìš”í•  ë•Œë§Œ ë¸”ë¡ì„ ë™ì  í• ë‹¹í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤(Near-zero waste).\n",
                "\n",
                "```\n",
                "Block 0 (Physical): [Token 0-3]\n",
                "Block 1 (Physical): [Token 4-7]\n",
                "Block Table (Logical): {Seq_A: [Block 0, Block 1, ...]}\n",
                "-> ë¹„ì—°ì†ì  í• ë‹¹ ê°€ëŠ¥, ìœ ì—°í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
                "\n",
                "```\n",
                "\n",
                "### 1.3 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 2: KV Cache ìµœì í™”\n",
                "\n",
                "LLMì€ ìê¸°íšŒê·€(Autoregressive) ëª¨ë¸ì´ë¯€ë¡œ, ì´ì „ í† í°ë“¤ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
                "\n",
                "**KV Cache ë¶€ì¬ ì‹œ (Redundant Computation)**:\n",
                "\n",
                "* ë§¤ ìŠ¤í…ë§ˆë‹¤ ì´ì „ ì‹œì ()ê¹Œì§€ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ Attention ì—°ì‚°ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
                "* ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ $O(N^2)$ë¡œ ì¦ê°€í•˜ì—¬ Latencyê°€ ê¸‰ê²©íˆ ë‚˜ë¹ ì§‘ë‹ˆë‹¤.\n",
                "\n",
                "**KV Cache ì ìš© ì‹œ (Memory-Space Tradeoff)**:\n",
                "\n",
                "* ì´ì „ í† í°ë“¤ì˜ Key, Value ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ VRAMì— ìºì‹±í•´ë‘¡ë‹ˆë‹¤.\n",
                "* í˜„ì¬ ì‹œì ()ì˜ í† í°ë§Œ ì—°ì‚°í•˜ê³ , ê³¼ê±° ë°ì´í„°ëŠ” ìºì‹œë¥¼ ì°¸ì¡°(Lookup)í•˜ì—¬ ì—°ì‚° ë³µì¡ë„ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
                "\n",
                "**Critical Issue**:\n",
                "\n",
                "* KV CacheëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ë§¤ìš° í° VRAMì„ ì ìœ í•©ë‹ˆë‹¤.\n",
                "* ê²°êµ­ **\"ì œí•œëœ GPU ë©”ëª¨ë¦¬ì— ì–¼ë§ˆë‚˜ ë§ì€ KV Cacheë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ê²¨ ë„£ëŠëƒ\"**ê°€ ì„œë¹™ ì„±ëŠ¥(Throughput)ì˜ ê²°ì •ì  ìš”ì†Œê°€ ë˜ë©°, ì´ê²ƒì´ vLLMì´ PagedAttentionì„ ë„ì…í•œ ê¸°ìˆ ì  ë°°ê²½ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ssafy/vllm-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "2026-02-24 17:07:20,283\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch ë²„ì „: 2.5.1+cu124\n",
                        "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
                        "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
                "import os\n",
                "import time\n",
                "import torch\n",
                "from vllm import LLM, SamplingParams\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
                "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4: PagedAttention ë©”ì»¤ë‹ˆì¦˜ ì‹œë®¬ë ˆì´ì…˜\n",
                "\n",
                "ì´ë¡ ì  ê°œë…ì„ êµ¬ì²´í™”í•˜ê¸° ìœ„í•´, Python í™˜ê²½ì—ì„œ **PagedAttentionì˜ ë©”ëª¨ë¦¬ í• ë‹¹ ë¡œì§**ì„ ì‹œë®¬ë ˆì´ì…˜í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°€ìƒ ë©”ëª¨ë¦¬ ì£¼ì†Œê°€ ì‹¤ì œ GPU ë©”ëª¨ë¦¬ì— ë§¤í•‘ë˜ëŠ” ê³¼ì •ì„ ì½”ë“œ ë ˆë²¨ì—ì„œ ê²€ì¦í•©ë‹ˆë‹¤.\n",
                "\n",
                "### ì‹œë®¬ë ˆì´í„° í•µì‹¬ ì»´í¬ë„ŒíŠ¸ (Simulator Components)\n",
                "\n",
                "* **Logical Memory (Virtual Address Space)**\n",
                "  * ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë…¼ë¦¬ì ì¸ í† í° ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤.\n",
                "  * ì‚¬ìš©ì ì…ì¥ì—ì„œëŠ” ë°ì´í„°ê°€ ì—°ì†ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* **Physical Memory (Physical Address Space)**\n",
                "  * ì‹¤ì œ KV Cache ë°ì´í„°ê°€ ì ì¬ë˜ëŠ” ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„(VRAM)ì…ë‹ˆë‹¤.\n",
                "  * ë¸”ë¡(Block) ë‹¨ìœ„ë¡œ ë¹„ì—°ì†ì ìœ¼ë¡œ í• ë‹¹ ë° ê´€ë¦¬ë©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* **Block Table (Address Translation)**\n",
                "  * Logical Blockì„ Physical Blockìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë§¤í•‘ í…Œì´ë¸”ì…ë‹ˆë‹¤.\n",
                "  * OSì˜ í˜ì´ì§€ í…Œì´ë¸”(Page Table)ê³¼ ë™ì¼í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ì°¸ì¡°ë¥¼ ì¤‘ê°œí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "class PhysicalBlock:\n",
                "    \"\"\"\n",
                "    ë¬¼ë¦¬ì ì¸ ë©”ëª¨ë¦¬ ë¸”ë¡ (GPUì˜ ì‘ì€ ì¡°ê°)\n",
                "    ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ í•˜ë‚˜ì˜ ë¸”ë¡ì— 4ê°œì˜ í† í°ì„ ì €ì¥í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤ (BLOCK_SIZE = 4).\n",
                "    \"\"\"\n",
                "    def __init__(self, block_id: int, block_size: int = 4):\n",
                "        self.block_id = block_id\n",
                "        self.size = block_size\n",
                "        self.data = [None] * block_size  # ì²˜ìŒì—” ë¹„ì–´ìˆìŒ\n",
                "        self.filled_count = 0\n",
                "\n",
                "    def append(self, token: str) -> bool:\n",
                "        if self.filled_count < self.size:\n",
                "            self.data[self.filled_count] = token\n",
                "            self.filled_count += 1\n",
                "            return True  # ì €ì¥ ì„±ê³µ\n",
                "        return False  # ê½‰ ì°¨ì„œ ì‹¤íŒ¨\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"[Block {self.block_id}]: {self.data}\"\n",
                "\n",
                "class VLLMMemoryManager:\n",
                "    \"\"\"\n",
                "    vLLMì˜ í•µì‹¬ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (OS ì—­í• )\n",
                "    \"\"\"\n",
                "    def __init__(self, total_blocks: int = 16, block_size: int = 4):\n",
                "        self.block_size = block_size\n",
                "        # 1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ê³µê°„ ìƒì„± (GPU VRAM)\n",
                "        self.physical_memory = [PhysicalBlock(i, block_size) for i in range(total_blocks)]\n",
                "        \n",
                "        # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ (Free List)\n",
                "        # ì‹¤ì œë¡œëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ë–¨ì–´ì ¸ ìˆì–´ë„ ìƒê´€ì—†ìŒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì„ìŠµë‹ˆë‹¤.\n",
                "        self.free_blocks = list(range(total_blocks))\n",
                "        random.shuffle(self.free_blocks) \n",
                "        \n",
                "        # 3. ìš”ì²­ë³„ ë¸”ë¡ ë§¤í•‘ í…Œì´ë¸” (Request ID -> List[Physical Block IDs])\n",
                "        self.block_tables: Dict[str, List[int]] = {}\n",
                "\n",
                "    def allocate_block(self) -> Optional[int]:\n",
                "        \"\"\"ë¹ˆ ë¬¼ë¦¬ ë¸”ë¡ í•˜ë‚˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
                "        if not self.free_blocks:\n",
                "            return None # OOM (Out of Memory)\n",
                "        return self.free_blocks.pop()\n",
                "\n",
                "    def append_token(self, request_id: str, token: str):\n",
                "        \"\"\"\n",
                "        í•µì‹¬ ë¡œì§: í† í°ì„ ìƒì„±í•´ì„œ KV Cacheì— ì¶”ê°€í•˜ëŠ” ê³¼ì •\n",
                "        \"\"\"\n",
                "        # 1. í•´ë‹¹ ìš”ì²­ì˜ ë¸”ë¡ í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±\n",
                "        if request_id not in self.block_tables:\n",
                "            print(f\"--- [New Request] '{request_id}' ì‹œì‘ ---\")\n",
                "            self.block_tables[request_id] = []\n",
                "\n",
                "        # 2. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë§ˆì§€ë§‰ ë¬¼ë¦¬ ë¸”ë¡ í™•ì¸\n",
                "        current_block_id = None\n",
                "        if self.block_tables[request_id]:\n",
                "            current_block_id = self.block_tables[request_id][-1]\n",
                "        \n",
                "        # 3. ë¸”ë¡ì´ ì—†ê±°ë‚˜ ê½‰ ì°¼ìœ¼ë©´, ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ í• ë‹¹ (PagedAttentionì˜ í•µì‹¬!)\n",
                "        if current_block_id is None or \\\n",
                "           self.physical_memory[current_block_id].filled_count >= self.block_size:\n",
                "            \n",
                "            new_block_id = self.allocate_block()\n",
                "            if new_block_id is None:\n",
                "                raise Exception(\"GPU Memory Full!\")\n",
                "            \n",
                "            self.block_tables[request_id].append(new_block_id)\n",
                "            current_block_id = new_block_id\n",
                "            print(f\"ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆ í• ë‹¹ë¨\")\n",
                "\n",
                "        # 4. ë¬¼ë¦¬ ë¸”ë¡ì— ë°ì´í„° ì €ì¥\n",
                "        self.physical_memory[current_block_id].append(token)\n",
                "        print(f\"   [Write] í† í° '{token}' -> ë¬¼ë¦¬ ë¸”ë¡ {current_block_id}ë²ˆì— ì €ì¥\")\n",
                "\n",
                "    def print_state(self, request_id: str):\n",
                "        \"\"\"í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì‹œê°í™”í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.\"\"\"\n",
                "        print(f\"\\nğŸ“Š [{request_id}] ì˜ PagedAttention ìƒíƒœ\")\n",
                "        table = self.block_tables.get(request_id, [])\n",
                "        \n",
                "        # 1. Logical View (ì‚¬ìš©ìê°€ ë³´ëŠ” ë¬¸ì¥)\n",
                "        logical_text = []\n",
                "        for block_id in table:\n",
                "            block = self.physical_memory[block_id]\n",
                "            logical_text.extend([t for t in block.data if t is not None])\n",
                "        print(f\"  1) ë…¼ë¦¬ì  ë·° (Logical): {logical_text}\")\n",
                "\n",
                "        # 2. Block Table (ë§¤í•‘ ì •ë³´)\n",
                "        print(f\"  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): {table}\")\n",
                "\n",
                "        # 3. Physical View (ì‹¤ì œ ì €ì¥ ìœ„ì¹˜)\n",
                "        print(f\"  3) ë¬¼ë¦¬ì  ë·° (Physical):\")\n",
                "        for block_id in table:\n",
                "            print(f\"     {self.physical_memory[block_id]}\")\n",
                "        print(\"-\" * 50)\n",
                "\n",
                "print(\"âœ… PagedAttention ì‹œë®¬ë ˆì´í„° í´ë˜ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\n",
                        "\n",
                        "--- [New Request] 'User_A' ì‹œì‘ ---\n",
                        "ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆ í• ë‹¹ë¨\n",
                        "   [Write] í† í° 'Deep' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'Learn' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'ing' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'is' -> ë¬¼ë¦¬ ë¸”ë¡ 1ë²ˆì— ì €ì¥\n",
                        "ğŸ‘‰ [Alloc] ìƒˆë¡œìš´ ë¬¼ë¦¬ ë¸”ë¡ 8ë²ˆ í• ë‹¹ë¨\n",
                        "   [Write] í† í° 'very' -> ë¬¼ë¦¬ ë¸”ë¡ 8ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'fun' -> ë¬¼ë¦¬ ë¸”ë¡ 8ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'to' -> ë¬¼ë¦¬ ë¸”ë¡ 8ë²ˆì— ì €ì¥\n",
                        "   [Write] í† í° 'study' -> ë¬¼ë¦¬ ë¸”ë¡ 8ë²ˆì— ì €ì¥\n",
                        "\n",
                        "ğŸ“Š [User_A] ì˜ PagedAttention ìƒíƒœ\n",
                        "  1) ë…¼ë¦¬ì  ë·° (Logical): ['Deep', 'Learn', 'ing', 'is', 'very', 'fun', 'to', 'study']\n",
                        "  2) ë¸”ë¡ í…Œì´ë¸” (Mapping): [1, 8]\n",
                        "  3) ë¬¼ë¦¬ì  ë·° (Physical):\n",
                        "     [Block 1]: ['Deep', 'Learn', 'ing', 'is']\n",
                        "     [Block 8]: ['very', 'fun', 'to', 'study']\n",
                        "--------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# PagedAttention ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
                "# LLMì´ í•œ í† í°ì”© ìƒì„±í•˜ëŠ” ìƒí™©ì„ í‰ë‚´ëƒ…ë‹ˆë‹¤\n",
                "\n",
                "# 1. ë§¤ë‹ˆì € ì´ˆê¸°í™” (Block Size = 4)\n",
                "vllm_manager = VLLMMemoryManager(total_blocks=10, block_size=4)\n",
                "\n",
                "# 2. ë¬¸ì¥ ìƒì„± ì‹œë®¬ë ˆì´ì…˜\n",
                "req_id = \"User_A\"\n",
                "tokens = [\"Deep\", \"Learn\", \"ing\", \"is\", \"very\", \"fun\", \"to\", \"study\"]\n",
                "\n",
                "print(\"ğŸš€ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\\n\")\n",
                "for token in tokens:\n",
                "    vllm_manager.append_token(req_id, token)\n",
                "\n",
                "# 3. ìµœì¢… ìƒíƒœ í™•ì¸\n",
                "vllm_manager.print_state(req_id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° ê¸°ìˆ ì  ì‹œì‚¬ì \n",
                "\n",
                "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ í†µí•´ PagedAttention ì•„í‚¤í…ì²˜ê°€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ëŠ” êµ¬ì²´ì ì¸ ë©”ì»¤ë‹ˆì¦˜ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ìˆ ì  íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
                "\n",
                "#### ğŸ”‘ Key Technical Insights\n",
                "\n",
                "**1. ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ì˜ ë¹„ì—°ì†ì„± (Non-contiguous Allocation)**\n",
                "\n",
                "* **ê´€ì°°**: ë…¼ë¦¬ì  ì‹œí€€ìŠ¤ìƒì—ì„œëŠ” `'is'`(Block 0ì˜ ë)ì™€ `'very'`(Block 1ì˜ ì‹œì‘)ê°€ ì—°ì†ë˜ì§€ë§Œ, ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ì£¼ì†Œ(Physical Block Index)ëŠ” ë¶ˆì—°ì†ì ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤.\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ë¬¼ë¦¬ì  ì£¼ì†Œ ê³µê°„ì˜ ì—°ì†ì„± ì œì•½ì„ ì œê±°í•¨ìœ¼ë¡œì¨, VRAM ë‚´ ì‚°ì¬í•œ ìœ íœ´ ë©”ëª¨ë¦¬ ì¡°ê°(Fragmented chunks)ì„ 100% í™œìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation) ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤.\n",
                "\n",
                "**2. í• ë‹¹ì˜ íš¨ìœ¨ì„± (On-demand Allocation)**\n",
                "\n",
                "* **ê´€ì°°**: ìƒˆë¡œìš´ ë¬¼ë¦¬ì  ë¸”ë¡ì€ í˜„ì¬ ë¸”ë¡ì´ ì™„ì „íˆ ì±„ì›Œì§„ ì‹œì (Slot full)ì—ë§Œ í• ë‹¹ë©ë‹ˆë‹¤.\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ê¸°ì¡´ì˜ ì •ì  í• ë‹¹ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” 'ì˜ˆì•½ëœ ë¯¸ì‚¬ìš© ê³µê°„(Reserved but unused memory)'ì— ì˜í•œ ë‚­ë¹„ë¥¼ ì œê±°í•©ë‹ˆë‹¤. ë¬¸ì¥ ìƒì„± ì¤‘ë‹¨ ì‹œ, ë¶ˆí•„ìš”í•œ ì¶”ê°€ ë©”ëª¨ë¦¬ í• ë‹¹ì´ ë°œìƒí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ VRAM ì ìœ ìœ¨(Footprint)ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
                "\n",
                "**3. ì£¼ì†Œ ë³€í™˜ ë§¤ì»¤ë‹ˆì¦˜ (Address Translation Logic)**\n",
                "\n",
                "* **ê´€ì°°**: vLLMì€ `Block Table`ì„ í†µí•´ ë…¼ë¦¬ì  ì¸ë±ìŠ¤ë¥¼ ë¬¼ë¦¬ì  ì£¼ì†Œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
                "* **Block Index**: `Global Token Index // Block Size`\n",
                "* **Block Offset**: `Global Token Index % Block Size`\n",
                "\n",
                "\n",
                "* **ê¸°ìˆ ì  ì˜ì˜**: ì´ ì—°ì‚° ê³¼ì •ì€ $O(1)$ì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ ê°€ì§€ë¯€ë¡œ, í† í° ì¡°íšŒ(Lookup) ì‹œ ì˜¤ë²„í—¤ë“œê°€ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ì–»ëŠ” ëŒ€ì‹  ê°ìˆ˜í•´ì•¼ í•  ì„±ëŠ¥ ë¹„ìš©(Trade-off)ì´ ë¬´ì‹œí•  ìˆ˜ì¤€ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2 vLLM ê¸°ë³¸ ì‚¬ìš©ë²•\n",
                "\n",
                "### 2.1 ëª¨ë¸ ë¡œë”©\n",
                "vLLMì˜ Inference Engine(LLM Class)ì„ ì´ˆê¸°í™”í•˜ëŠ” ì§„ì…ì (Entry Point)ì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ì „ëµê³¼ ëª¨ë¸ ë¡œë”© ë°©ì‹ì´ ê²°ì •ë˜ë¯€ë¡œ, ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”ì— ìˆì–´ ê°€ì¥ ì¤‘ìš”í•œ ì„¤ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "### vLLM Engine Initialization Parameters\n",
                "\n",
                "#### 1. `tensor_parallel_size` (TP Degree)\n",
                "\n",
                "ì •ì˜: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(Weights)ì™€ ì—°ì‚°ì„ ëª‡ ê°œì˜ GPUì— ë¶„ì‚°í•˜ì—¬ ì²˜ë¦¬í• ì§€ ê²°ì •í•˜ëŠ” Tensor Parallelismì˜ ì°¨ìˆ˜(Degree)ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, GPUê°€ 4ì¥ ë“¤ì–´ìˆì„ ê²½ìš° tensor_parallel_size=4 ì…ë‹ˆë‹¤. í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„œëŠ” gpuê°€ 1ê°œë°–ì— ì—†ê¸° ë•Œë¬¸ì— tensor_parallel_size=1 ê³ ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "* Megatron-LM ìŠ¤íƒ€ì¼ì˜ Tensor Model Parallelismì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "* ê° Transformer ë ˆì´ì–´ì˜ í–‰ë ¬ ì—°ì‚°(Matrix Multiplication)ì„ ë¬¼ë¦¬ì  GPUë“¤ì— ìˆ˜í‰ì ìœ¼ë¡œ ìƒ¤ë”©(Sharding)í•©ë‹ˆë‹¤.\n",
                "* ì˜ˆ: ì¸ ê²½ìš°, í–‰ë ¬ ë¥¼ ë¡œ ìª¼ê°œì–´ ë‘ GPUê°€ ê°ê° ì—°ì‚°í•œ í›„, `All-Reduce` í†µì‹ ì„ í†µí•´ ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* `value=1`: ë‹¨ì¼ GPU ëª¨ë“œì…ë‹ˆë‹¤. ëª¨ë¸ ì „ì²´ê°€ í•˜ë‚˜ì˜ GPU VRAMì— ì ì¬ ê°€ëŠ¥í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. í†µì‹  ì˜¤ë²„í—¤ë“œê°€ ì—†ì–´ Latency ì¸¡ë©´ì—ì„œ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "* `value > 1`: ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì»¤ì„œ ë‹¨ì¼ GPUì— ì ì¬ê°€ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜(OOM), ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê³ ì í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "* Constraint: ë°˜ë“œì‹œ ë¨¸ì‹ ì— ì¥ì°©ëœ ë¬¼ë¦¬ì  GPU ê°œìˆ˜ ì´í•˜ì—¬ì•¼ í•˜ë©°, GPU ê°„ í†µì‹  ëŒ€ì—­í­(NVLink ë“±)ì´ ì„±ëŠ¥ì˜ ë³‘ëª©ì´ ë  ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 2. `gpu_memory_utilization` (VRAM Budgeting)\n",
                "\n",
                "ì •ì˜: vLLM í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ VRAM ë¹„ìœ¨(Fraction)ì„ ì„¤ì •í•©ë‹ˆë‹¤. (Default: 0.9) ë§Œì•½, 6GBì˜ VRAM GPUì—ì„œ 0.9ë¥¼ ì„¤ì •í•˜ë©´ 6 * 0.9 = 5.4GBì˜ gpuë¥¼ ë¯¸ë¦¬ í• ë‹¹í•©ë‹ˆë‹¤. ì‹¤ì œ ëª¨ë¸ì˜ í¬ê¸°ê°€ 3GBë”ë¼ë„ 5.4GBë¥¼ ì°¨ì§€í•˜ê²Œ ë˜ë¯€ë¡œ ë„ˆë¬´ ë†’ì€ ê°’ì„ ì„¤ì •í•˜ê²Œ ë˜ë©´ gpu ë‚­ë¹„ê°€ ë˜ê³ , ë°˜ë©´ì— ë„ˆë¬´ ì ê²Œ ì„¤ì •í•˜ë©´ cpu ì¶”ë¡ ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ë•Œë¬¸ì— ë„ˆë¬´ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.\n",
                "\n",
                "* ì—”ì§„ ì´ˆê¸°í™” ì‹œ Profiling Phaseê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
                "* `(ì „ì²´ VRAM * 0.9) - (ëª¨ë¸ ê°€ì¤‘ì¹˜ + Activation ì˜¤ë²„í—¤ë“œ)`ë¥¼ ê³„ì‚°í•˜ì—¬, ë‚¨ì€ ê³µê°„ì„ KV Cacheìš© ë¸”ë¡ìœ¼ë¡œ ë¯¸ë¦¬ í• ë‹¹(Pre-allocation)í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Why not 1.0?: PyTorch ëŸ°íƒ€ì„ì´ë‚˜ CUDA Contextê°€ ì‚¬ìš©í•  ì—¬ìœ  ê³µê°„(Buffer)ì„ ë‚¨ê²¨ë‘¬ì•¼ í•©ë‹ˆë‹¤. 1.0ìœ¼ë¡œ ì„¤ì • ì‹œ OOM(Out of Memory)ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ê°€ ê°•ì œ ì¢…ë£Œë  ìœ„í—˜ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\n",
                "* Performance Trade-off: ì´ ê°’ì„ ë†’ì´ë©´ KV Cache ê³µê°„ì´ ëŠ˜ì–´ë‚˜ ë°°ì¹˜ í¬ê¸°(Batch Size)ë¥¼ í‚¤ìš¸ ìˆ˜ ìˆì–´ ì²˜ë¦¬ëŸ‰(Throughput)ì´ í–¥ìƒë©ë‹ˆë‹¤. ë°˜ë©´, ë„ˆë¬´ ë†’ê²Œ ì¡ìœ¼ë©´ ëŸ°íƒ€ì„ ì¤‘ í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ì¶©ëŒì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 0.9~0.95 ì‚¬ì´ì—ì„œ íŠœë‹í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 3. `trust_remote_code` (Execution Policy)\n",
                "\n",
                "ì •ì˜: HuggingFace Hub ë“± ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ì»¤ìŠ¤í…€ Python ì½”ë“œ(`modeling_*.py`) ì‹¤í–‰ì„ í—ˆìš©í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤.\n",
                "\n",
                "* `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì•„ì§ ê³µì‹ í†µí•©(Upstream)ë˜ì§€ ì•Šì€ ìµœì‹  ì•„í‚¤í…ì²˜ë‚˜, ì»¤ìŠ¤í…€ ëª¨ë¸(ì˜ˆ: MPT, Falcon ì´ˆê¸° ë²„ì „ ë“±)ì„ ë¡œë“œí•  ë•Œ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "* `True`ë¡œ ì„¤ì • ì‹œ, ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ê¹Œì§€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Security Risk: ê²€ì¦ë˜ì§€ ì•Šì€ ëª¨ë¸ ì €ì¥ì†Œì—ì„œ ì´ ì˜µì…˜ì„ ì¼œë©´ ì•…ì„± ì½”ë“œê°€ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” RCE(Remote Code Execution) ì·¨ì•½ì ì— ë…¸ì¶œë©ë‹ˆë‹¤.\n",
                "* Best Practice: ê²€ì¦ëœ ê³µì‹ ë¦¬í¬ì§€í† ë¦¬(Official Org)ë‚˜ ì‚¬ë‚´ ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš°, ì½”ë“œë¥¼ ë¨¼ì € ê²€í† (Audit)í•œ í›„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì›ì¹™ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ Llama 2, Mistral ê°™ì€ í‘œì¤€ ì•„í‚¤í…ì²˜ëŠ” `False`ë¡œ ì„¤ì •í•´ë„ vLLM ë‚´ë¶€ êµ¬í˜„ì²´ë¡œ ë¡œë”© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "#### 4. `enforce_eager` (CUDA Graph Strategy)\n",
                "\n",
                "**ì •ì˜:** vLLMì˜ í•µì‹¬ ìµœì í™” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì¸ **CUDA Graphs** ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. `True`ë¡œ ì„¤ì •í•˜ë©´ CUDA Graphsë¥¼ ë„ê³ , PyTorchì˜ ê¸°ë³¸ ì‹¤í–‰ ë°©ì‹ì¸ **Eager Mode**ë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤. (Default: `False`)\n",
                "\n",
                "* **CUDA Graphsë€?:** GPU ì—°ì‚° ì»¤ë„(Kernel)ë“¤ì˜ ì‹¤í–‰ ìˆœì„œì™€ ë©”ëª¨ë¦¬ í• ë‹¹ì„ ë¯¸ë¦¬ 'ê·¸ë˜í”„' í˜•íƒœë¡œ ìº¡ì²˜(Capture)í•´ë‘ê³ , ì¶”ë¡  ì‹œì—ëŠ” ì´ ê·¸ë˜í”„ë¥¼ í†µì§¸ë¡œ ì‹¤í–‰(Replay)í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ CPUê°€ GPUì— ëª…ë ¹ì„ ë‚´ë¦¬ëŠ” ì˜¤ë²„í—¤ë“œ(Kernel Launch Overhead)ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
                "* **Why use `True` (Disabling Graphs)?:**\n",
                "    * **Debugging:** CUDA ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œ ì •í™•í•œ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "    * **Memory Issue:** CUDA Graphë¥¼ ìº¡ì²˜í•˜ê³  ìœ ì§€í•˜ëŠ” ë°ì—ë„ ì¼ì •ëŸ‰ì˜ VRAMì´ ì†Œëª¨ë©ë‹ˆë‹¤. VRAMì´ ê·¹í•œìœ¼ë¡œ ë¶€ì¡±í•˜ì—¬ OOMì´ ë°œìƒí•  ë•Œ, ì„±ëŠ¥ ì €í•˜ë¥¼ ê°ìˆ˜í•˜ê³  ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ ì¼¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "    * **Hardware Compatibility:** ì¼ë¶€ êµ¬í˜• GPUë‚˜ íŠ¹ì • ë“œë¼ì´ë²„ í™˜ê²½ì—ì„œ CUDA Graphsê°€ ë¶ˆì•ˆì •í•  ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "    * **Performance Impact:** `True`ë¡œ ì„¤ì •í•  ê²½ìš°, CPU ì˜¤ë²„í—¤ë“œê°€ ì¦ê°€í•˜ì—¬ íŠ¹íˆ ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆì—ì„œì˜ ì²˜ë¦¬ëŸ‰(Throughput)ê³¼ ì§€ì—° ì‹œê°„(Latency) ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” `False`(ê¸°ë³¸ê°’) ìœ ì§€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tue Feb 24 17:07:21 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 590.52.01              Driver Version: 591.74         CUDA Version: 13.1     |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
                        "| N/A   51C    P4             12W /   30W |       0MiB /   6141MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "\n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|  No running processes found                                                             |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\n",
                        "INFO 02-24 17:07:32 config.py:510] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
                        "WARNING 02-24 17:07:32 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
                        "WARNING 02-24 17:07:32 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
                        "INFO 02-24 17:07:32 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=slow, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
                        "WARNING 02-24 17:07:32 tokenizer.py:174] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
                        "WARNING 02-24 17:07:33 interface.py:236] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
                        "INFO 02-24 17:07:33 selector.py:120] Using Flash Attention backend.\n",
                        "INFO 02-24 17:07:34 model_runner.py:1094] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n",
                        "INFO 02-24 17:07:34 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
                        "INFO 02-24 17:07:35 weight_utils.py:296] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 17:07:35 model_runner.py:1099] Loading model weights took 0.6749 GB\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 17:07:36 worker.py:241] Memory profiling takes 0.96 seconds\n",
                        "INFO 02-24 17:07:36 worker.py:241] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.75) = 4.50GiB\n",
                        "INFO 02-24 17:07:36 worker.py:241] model weights take 0.67GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 3.27GiB.\n",
                        "INFO 02-24 17:07:37 gpu_executor.py:76] # GPU blocks: 5365, # CPU blocks: 6553\n",
                        "INFO 02-24 17:07:37 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 10.48x\n",
                        "INFO 02-24 17:07:37 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 1.66 seconds\n",
                        "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (15.86ì´ˆ)\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "from vllm import LLM, SamplingParams\n",
                "\n",
                "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "\n",
                "print(\"ğŸš€ vLLM ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "llm = LLM(\n",
                "    model=model_name,\n",
                "    tensor_parallel_size=1,\n",
                "    gpu_memory_utilization=0.75,\n",
                "    trust_remote_code=True,\n",
                "    enforce_eager=True,\n",
                "    tokenizer_mode=\"slow\",\n",
                ")\n",
                "\n",
                "load_time = time.time() - start_time\n",
                "print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({load_time:.2f}ì´ˆ)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/bin/bash: line 1: nvidia-smiz: command not found\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smiz\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤ìŠµ\n",
                "\n",
                "ì•„ë˜ ì½”ë“œëŠ” LLMì˜ Decoding Strategy(ë””ì½”ë”© ì „ëµ)ë¥¼ ì •ì˜í•˜ëŠ” `SamplingParams` ê°ì²´ ìƒì„±ë¶€ì…ë‹ˆë‹¤. ëª¨ë¸ì´ í™•ë¥  ë¶„í¬(Logits)ë¡œë¶€í„° ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ì œì–´í•˜ì—¬, ìƒì„± ê²°ê³¼ì˜ í’ˆì§ˆ, ë‹¤ì–‘ì„±, ê·¸ë¦¬ê³  ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ê²°ì •ì§“ëŠ” í•µì‹¬ íŒŒë¼ë¯¸í„°ë“¤ì…ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "### Decoding Strategy Configuration\n",
                "\n",
                "#### 1. `max_tokens` (Termination Condition)\n",
                "\n",
                "ì •ì˜: ëª¨ë¸ì´ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´(Maximum Output Length)ë¥¼ ì œí•œí•˜ëŠ” Hard Limitì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, output í† í° ìˆ˜ê°€ 128ì— ë„ë‹¬í•˜ë©´ 128ì—ì„œ ì¤‘ê°„ì— ìƒì„±ì„ ë©ˆì¶¥ë‹ˆë‹¤.\n",
                "\n",
                "* Latency Budgeting: ìƒì„± í† í° ìˆ˜ëŠ” ì¶”ë¡  ì‹œê°„(Inference Latency)ê³¼ ì •ë¹„ë¡€í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ì˜ ì‘ë‹µ ì†ë„ ëª©í‘œ(SLA)ì— ë§ì¶° ì ì ˆí•œ ìƒí•œì„ ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "* Resource Management: ìƒì„± ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ KV Cache ì ìœ ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤. ë¬´í•œ ìƒì„±ì„ ë°©ì§€í•˜ì—¬ OOM(Out of Memory) ë° ì—°ì‚° ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ë¥¼ ë§‰ëŠ” ì•ˆì „ì¥ì¹˜ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
                "* Note: ì´ ê°’ì— ë„ë‹¬í•˜ë©´ ë¬¸ì¥ì´ ì™„ê²°ë˜ì§€ ì•Šì•˜ë”ë¼ë„ ê°•ì œë¡œ ìƒì„±ì„ ì¤‘ë‹¨(Stop)í•©ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "\n",
                "#### 2. `temperature` (Entropy Scaling)\n",
                "\n",
                "ì •ì˜: Softmax í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ê¸° ì „, Logit ê°’ì„ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ í™•ë¥  ë¶„í¬ì˜ í‰íƒ„ë„(Flatness)ë¥¼ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.\n",
                "\n",
                "* Low (): í™•ë¥  ë¶„í¬ë¥¼ ë‚ ì¹´ë¡­ê²Œ(Peaky) ë§Œë“­ë‹ˆë‹¤. í™•ë¥ ì´ ë†’ì€ í† í°ì€ ë” ë†’ì•„ì§€ê³ , ë‚®ì€ í† í°ì€ ë” ë‚®ì•„ì ¸ ê²°ì •ë¡ ì (Deterministic)ì´ê³  ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ ë„í•©ë‹ˆë‹¤. (Code generation ë“±ì— ì í•©)\n",
                "* High (): í™•ë¥  ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ(Flatten) ë§Œë“­ë‹ˆë‹¤. í™•ë¥ ì´ ë‚®ì€ í† í°ì˜ ì„ íƒ ê°€ëŠ¥ì„±ì„ ë†’ì—¬ ë‹¤ì–‘ì„±(Diversity)ê³¼ ì°½ì˜ì„±ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. (Creative writing ë“±ì— ì í•©)\n",
                "\n",
                "\n",
                "\n",
                "#### 3. `top_p` (Nucleus Sampling)\n",
                "\n",
                "ì •ì˜: í™•ë¥ ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ í† í°ì„ ì •ë ¬í•œ ë’¤, ëˆ„ì  í™•ë¥ (Cumulative Probability)ì´ ê°’(ì—¬ê¸°ì„œëŠ” 0.9, ì¦‰ 90%)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ë§Œ í›„ë³´êµ°(Candidate Set)ì— í¬í•¨ì‹œí‚¤ëŠ” ë™ì  ì ˆë‹¨(Dynamic Truncation) ë°©ì‹ì…ë‹ˆë‹¤. tokenizerì— ìˆëŠ” vocabì—ì„œ ìƒì„±í•  í›„ë³´ë“¤ì„ ê³ ë ¤í•˜ê²Œ ë˜ëŠ”ë° 90%ë¡œ ì„¤ì •í•˜ë©´ ë‚˜ë¨¸ì§€ 10%ì˜ í™•ë¥ ì„ ì°¨ì§€í•˜ëŠ” ìˆ˜ë§ì€ í† í°ë“¤ì€ ê³ ë ¤í•˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, vocabì´ 128,000ì´ê³ , \"ì•ˆë…•\" ì´ë¼ëŠ” í† í°ì˜ í™œë¥ ì´ 90% ë¼ê³  í•˜ë©´ ë‚˜ë¨¸ì§€ 127,999ì˜ í† í°ì€ ê³ ë ¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
                "\n",
                "* ëª¨ë¸ì´ í™•ì‹ ì„ ê°€ì§€ëŠ” ìƒí™©(íŠ¹ì • ë‹¨ì–´ì˜ í™•ë¥ ì´ ë§¤ìš° ë†’ìŒ)ì—ì„œëŠ” í›„ë³´êµ°ì„ ì¢ê²Œ ê°€ì ¸ê°€ê³ ,\n",
                "* í™•ì‹ ì´ ì—†ëŠ” ìƒí™©(ì—¬ëŸ¬ ë‹¨ì–´ì˜ í™•ë¥ ì´ ë¹„ìŠ·í•¨)ì—ì„œëŠ” í›„ë³´êµ°ì„ ë„“ê²Œ ê°€ì ¸ê°‘ë‹ˆë‹¤.\n",
                "\n",
                "\n",
                "* Engineering Insight: `top_k`ì˜ ê²½ì§ì„±ì„ ë³´ì™„í•˜ì—¬, ë¬¸ë§¥ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ íƒìƒ‰ ê³µê°„(Search Space)ì„ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ í…ìŠ¤íŠ¸ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ í’ë¶€í•œ í‘œí˜„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
                "\n",
                "#### 4. `top_k` (Top-K Sampling)\n",
                "\n",
                "ì •ì˜: ë‹¤ìŒ í† í° í›„ë³´ë¥¼ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ ê°œ(ì—¬ê¸°ì„œëŠ” 50ê°œ)ë¡œ ê³ ì •í•˜ì—¬ ì œí•œí•˜ëŠ” ì •ì  ì ˆë‹¨(Static Truncation) ë°©ì‹ì…ë‹ˆë‹¤. top_pì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥ì´ë¼ê³  ìƒê°í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤.\n",
                "\n",
                "* Mechanism: 50 ë²ˆì§¸ ì´í›„ì˜ í† í°ë“¤ì€ í™•ë¥ ì´ ì•„ë¬´ë¦¬ ë†’ì•„ë„(í˜¹ì€ ì¡´ì¬í•˜ë”ë¼ë„) í›„ë³´êµ°ì—ì„œ ë°°ì œ(Masking)ë©ë‹ˆë‹¤.\n",
                "* í™•ë¥  ë¶„í¬ì˜ Long Tail(í™•ë¥ ì´ ë§¤ìš° ë‚®ì€ ê¼¬ë¦¬ ë¶€ë¶„)ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.\n",
                "* ë¬¸ë§¥ê³¼ ì „í˜€ ìƒê´€ì—†ëŠ” ì—‰ëš±í•œ í† í°(Outlier)ì´ ì„ íƒë  ê°€ëŠ¥ì„±ì„ ì›ì²œ ì°¨ë‹¨í•˜ì—¬ ìƒì„± ê²°ê³¼ì˜ ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\n",
                        "  - Max Tokens: 128\n",
                        "  - Temperature: 0.7\n",
                        "  - Top-p: 0.9\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: Sampling íŒŒë¼ë¯¸í„° ì„¤ì •\n",
                "sampling_params = SamplingParams(\n",
                "    max_tokens=128,        # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
                "    temperature=0.7,       # ìƒ˜í”Œë§ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€)\n",
                "    top_p=0.9,            # Nucleus sampling\n",
                "    top_k=50,             # Top-k sampling\n",
                ")\n",
                "\n",
                "print(\"âš™ï¸  Sampling íŒŒë¼ë¯¸í„°:\")\n",
                "print(f\"  - Max Tokens: {sampling_params.max_tokens}\")\n",
                "print(f\"  - Temperature: {sampling_params.temperature}\")\n",
                "print(f\"  - Top-p: {sampling_params.top_p}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– ì¶”ë¡  ì‹œì‘...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.43it/s, est. speed input: 9.99 toks/s, output: 85.19 toks/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… ì¶”ë¡  ì™„ë£Œ! (2.11ì´ˆ)\n",
                        "ğŸ“Š ì²˜ë¦¬ëŸ‰: 1.42 prompts/sec\n",
                        "\n",
                        "[í”„ë¡¬í”„íŠ¸ 1] Explain quantum computing in simple terms:\n",
                        "[ìƒì„± ê²°ê³¼] \n",
                        "\n",
                        "Quantum computers work by using the principles of quantum mechanics to perform calculations. They use quantum bits or qubits, which are like the basic units of quantum computing. Qubits can exist in multiple states at once, thanks to a property called superposition. They also have the ability to be in a state of \"zero\" and \"one\" simultaneously, thanks to entanglement.\n",
                        "\n",
                        "A key advantage of quantum computing is that it can perform calculations much faster than classical computers. This is because quantum computers can take advantage of the unique properties of qubits, which allow them to process information in ways that classical computers cannot.\n",
                        "\n",
                        "For example\n",
                        "[í† í° ìˆ˜] 128\n",
                        "------------------------------------------------------------\n",
                        "[í”„ë¡¬í”„íŠ¸ 2] What is the capital of France?\n",
                        "[ìƒì„± ê²°ê³¼] \n",
                        "[í† í° ìˆ˜] 1\n",
                        "------------------------------------------------------------\n",
                        "[í”„ë¡¬í”„íŠ¸ 3] Write a haiku about programming:\n",
                        "[ìƒì„± ê²°ê³¼] \n",
                        "\n",
                        "Code, language, algorithm\n",
                        "In a world of code and data\n",
                        "A programmer's life is simple\n",
                        "\n",
                        "This haiku captures the essence of programming: the language, the algorithm, the programming language, and the programmer's life.\n",
                        "[í† í° ìˆ˜] 50\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Step 3: ì¶”ë¡  ì‹¤í–‰\n",
                "prompts = [\n",
                "    \"Explain quantum computing in simple terms:\",\n",
                "    \"What is the capital of France?\",\n",
                "    \"Write a haiku about programming:\",\n",
                "]\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[í”„ë¡¬í”„íŠ¸ {i+1}] {prompts[i]}\")\n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2.5: KV Cache ì„±ëŠ¥ ë¹„êµ ì‹œë®¬ë ˆì´í„°\n",
                "\n",
                "ì´ì œ **ì™œ KV Cacheê°€ í•„ìš”í•œì§€** ì—°ì‚° íšŸìˆ˜ë¥¼ ì¹´ìš´íŒ…í•˜ì—¬ ì§ì ‘ í™•ì¸í•´ë´…ì‹œë‹¤\n",
                "\n",
                "ì´ ì‹œë®¬ë ˆì´í„°ëŠ” ì‹¤ì œ í–‰ë ¬ ì—°ì‚° ëŒ€ì‹ , 'ì—°ì‚°ì„ ëª‡ ë²ˆ ìˆ˜í–‰í–ˆëŠ”ê°€'ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "class ModelSimulator:\n",
                "    \"\"\"LLM ì¶”ë¡  ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
                "    def __init__(self):\n",
                "        self.op_count = 0  # ì—°ì‚° íšŸìˆ˜ ì¹´ìš´í„°\n",
                "\n",
                "    def compute_token_representation(self, token):\n",
                "        \"\"\"\n",
                "        í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜ (Embedding + Q, K, V ìƒì„±)\n",
                "        ê³ ë¹„ìš© ì—°ì‚°ì´ë¼ê³  ê°€ì •\n",
                "        \"\"\"\n",
                "        self.op_count += 1\n",
                "        return f\"Vector({token})\"\n",
                "\n",
                "    def attention_calculation(self, current_token, past_vectors):\n",
                "        \"\"\"\n",
                "        í˜„ì¬ í† í°ê³¼ ê³¼ê±° í† í°ë“¤ ê°„ì˜ ê´€ê³„ ê³„ì‚° (Attention)\n",
                "        \"\"\"\n",
                "        self.op_count += len(past_vectors)\n",
                "\n",
                "print(\"âœ… ModelSimulator ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\n",
                        "  Step 1: ë¬¸ì¥ ê¸¸ì´ 4 -> ëˆ„ì  ì—°ì‚°: 8\n",
                        "  Step 2: ë¬¸ì¥ ê¸¸ì´ 5 -> ëˆ„ì  ì—°ì‚°: 18\n",
                        "  Step 3: ë¬¸ì¥ ê¸¸ì´ 6 -> ëˆ„ì  ì—°ì‚°: 30\n",
                        "  Step 4: ë¬¸ì¥ ê¸¸ì´ 7 -> ëˆ„ì  ì—°ì‚°: 44\n",
                        "  Step 5: ë¬¸ì¥ ê¸¸ì´ 8 -> ëˆ„ì  ì—°ì‚°: 60\n",
                        "\n",
                        "ì´ ì—°ì‚° íšŸìˆ˜: 60\n"
                    ]
                }
            ],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 1: KV Cache ì—†ì´ ìƒì„± (No Cache)\n",
                "# ==========================================\n",
                "print(\"ğŸš« [Scenario 1] KV Cache ë¯¸ì‚¬ìš© (ë§¤ë²ˆ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°)\")\n",
                "model_no_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "input_prompt = [\"Deep\", \"Learning\", \"is\", \"Fun\"]\n",
                "\n",
                "for step in range(5):\n",
                "    current_context = input_prompt + generated_tokens\n",
                "    \n",
                "    # ë§¤ë²ˆ ëª¨ë“  í† í°ì„ ë‹¤ì‹œ ê³„ì‚°\n",
                "    vectors = []\n",
                "    for token in current_context:\n",
                "        vec = model_no_cache.compute_token_representation(token)\n",
                "        vectors.append(vec)\n",
                "    \n",
                "    model_no_cache.attention_calculation(\"New_Token\", vectors)\n",
                "    \n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    print(f\"  Step {step+1}: ë¬¸ì¥ ê¸¸ì´ {len(current_context)} -> ëˆ„ì  ì—°ì‚°: {model_no_cache.op_count}\")\n",
                "\n",
                "final_cost_no_cache = model_no_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_no_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\n",
                        "  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\n",
                        "  Step 1: ìºì‹œ í¬ê¸° 5 -> ëˆ„ì  ì—°ì‚°: 9\n",
                        "  Step 2: ìºì‹œ í¬ê¸° 6 -> ëˆ„ì  ì—°ì‚°: 15\n",
                        "  Step 3: ìºì‹œ í¬ê¸° 7 -> ëˆ„ì  ì—°ì‚°: 22\n",
                        "  Step 4: ìºì‹œ í¬ê¸° 8 -> ëˆ„ì  ì—°ì‚°: 30\n",
                        "  Step 5: ìºì‹œ í¬ê¸° 9 -> ëˆ„ì  ì—°ì‚°: 39\n",
                        "\n",
                        "ì´ ì—°ì‚° íšŸìˆ˜: 39\n"
                    ]
                }
            ],
            "source": [
                "# ==========================================\n",
                "# ì‹œë‚˜ë¦¬ì˜¤ 2: KV Cache ì‚¬ìš© (With Cache)\n",
                "# ==========================================\n",
                "print(\"\\nâœ… [Scenario 2] KV Cache ì‚¬ìš© (ê³¼ê±°ëŠ” ì €ì¥í•´ë‘ê³  ì¬ì‚¬ìš©)\")\n",
                "model_with_cache = ModelSimulator()\n",
                "generated_tokens = []\n",
                "kv_cache = []  # ì´ê²ƒì´ ë°”ë¡œ KV Cache!\n",
                "\n",
                "# Prefill: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (í•œ ë²ˆë§Œ)\n",
                "print(\"  [Prefill] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘...\")\n",
                "for token in input_prompt:\n",
                "    vec = model_with_cache.compute_token_representation(token)\n",
                "    kv_cache.append(vec)\n",
                "\n",
                "# Decode: í† í° ìƒì„±\n",
                "for step in range(5):\n",
                "    # ìƒˆë¡œìš´ í† í° í•˜ë‚˜ë§Œ ì²˜ë¦¬\n",
                "    current_vec = model_with_cache.compute_token_representation(\"New_Token\")\n",
                "    \n",
                "    # ê³¼ê±° cache + í˜„ì¬ ë²¡í„°ë¡œ attention\n",
                "    model_with_cache.attention_calculation(\"New_Token\", kv_cache)\n",
                "    \n",
                "    # ìºì‹œì— ì €ì¥\n",
                "    kv_cache.append(current_vec)\n",
                "    generated_tokens.append(f\"Gen_{step}\")\n",
                "    \n",
                "    print(f\"  Step {step+1}: ìºì‹œ í¬ê¸° {len(kv_cache)} -> ëˆ„ì  ì—°ì‚°: {model_with_cache.op_count}\")\n",
                "\n",
                "final_cost_with_cache = model_with_cache.op_count\n",
                "print(f\"\\nì´ ì—°ì‚° íšŸìˆ˜: {final_cost_with_cache}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\n",
                        "==================================================\n",
                        "1. No Cache Cost  : 60 (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\n",
                        "2. With Cache Cost: 39 (O(N) - ì„ í˜•ì  ì¦ê°€)\n",
                        "ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ 1.5ë°° ë” ì ì€ ì—°ì‚°!\n",
                        "==================================================\n"
                    ]
                }
            ],
            "source": [
                "# ìµœì¢… ë¹„êµ\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"ğŸ“Š ìµœì¢… ì—°ì‚° ë¹„ìš© ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¦„)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"1. No Cache Cost  : {final_cost_no_cache} (O(NÂ²) - ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)\")\n",
                "print(f\"2. With Cache Cost: {final_cost_with_cache} (O(N) - ì„ í˜•ì  ì¦ê°€)\")\n",
                "print(f\"ğŸš€ íš¨ìœ¨ í–¥ìƒ: ì•½ {final_cost_no_cache / final_cost_with_cache:.1f}ë°° ë” ì ì€ ì—°ì‚°!\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Performance Benchmark & Analysis (ê²°ê³¼ ë¶„ì„)\n",
                "\n",
                "ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ëŠ” **KV Cache ë©”ì»¤ë‹ˆì¦˜ì˜ ìœ ë¬´**ì™€ **vLLMì˜ ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ**ì´ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ê²°ì •ì ì¸(Critical) ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
                "\n",
                "#### 1. Naive Decoding (No Cache)\n",
                "\n",
                "* **Computational Complexity**:  (Quadratic)\n",
                "* **Analysis**: ë§¤ í† í° ìƒì„± ì‹œì ë§ˆë‹¤ ì´ì „ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ Attention ì—°ì‚°ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰(Re-computation)í•©ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´(Context Length)ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì—°ì‚° ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ì—¬, ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ê°€ ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ Latencyê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "\n",
                "#### 2. Standard KV Caching\n",
                "\n",
                "* **Computational Complexity**:  (Linear)\n",
                "* **Analysis**: ì „í˜•ì ì¸ **Space-Time Trade-off** ì „ëµì…ë‹ˆë‹¤. ê³¼ê±° í† í°ì˜ Key/Value Stateë¥¼ VRAMì— ìƒì£¼ì‹œì¼œ ì¤‘ë³µ ì—°ì‚°ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤. ì†ë„ëŠ” íšê¸°ì ìœ¼ë¡œ ê°œì„ ë˜ì§€ë§Œ, ê·¸ ëŒ€ê°€ë¡œ VRAM ì ìœ ìœ¨(Footprint)ì´ ê¸‰ì¦í•˜ì—¬ ë°°ì¹˜ í¬ê¸°(Batch Size) í™•ì¥ì— ë¬¼ë¦¬ì  ì œì•½ì´ ìƒê¹ë‹ˆë‹¤.\n",
                "\n",
                "#### 3. vLLM (PagedAttention)\n",
                "\n",
                "* **Mechanism**: **Zero-Waste Memory Management**\n",
                "* **Analysis**: KV Cacheì˜ ì—°ì‚° íš¨ìœ¨ì„±(O(N))ì€ ìœ ì§€í•˜ë˜, OS í˜ì´ì§• ê¸°ë²•ì„ í†µí•´ **ë©”ëª¨ë¦¬ ë‹¨í¸í™”(Fragmentation)** ë¬¸ì œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.\n",
                "* **Impact**: ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ì˜ ìœ íœ´ ê³µê°„ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¹ˆí‹ˆì—†ì´ í™œìš©(Packing)í•¨ìœ¼ë¡œì¨, ë™ì¼í•œ í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ ë‚´ì—ì„œ ìˆ˜ìš© ê°€ëŠ¥í•œ **KV Cacheì˜ ë°€ë„(Density)**ë¥¼ ë†’ì…ë‹ˆë‹¤. ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ë™ì‹œ ì²˜ë¦¬ëŸ‰(Throughput)ì˜ ê·¹ëŒ€í™”ë¡œ ì§ê²°ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3 Chat í˜•ì‹ í”„ë¡¬í”„íŠ¸\n",
                "\n",
                "ì‹¤ì „ì—ì„œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ìœ ì € ë©”ì‹œì§€, ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•œ chat í˜•ì‹ì„ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "def format_chat_messages(messages):\n",
                "    \"\"\"\n",
                "    Chat ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\n",
                "    \n",
                "    Args:\n",
                "        messages: [{'role': 'system/user/assistant', 'content': '...'}, ...]\n",
                "    \"\"\"\n",
                "    if tokenizer.chat_template:\n",
                "        # Tokenizerì— chat templateì´ ìˆìœ¼ë©´ ì‚¬ìš©\n",
                "        return tokenizer.apply_chat_template(\n",
                "            messages,\n",
                "            tokenize=False,\n",
                "            add_generation_prompt=True\n",
                "        )\n",
                "    else:\n",
                "        # ìˆ˜ë™ í¬ë§·íŒ…\n",
                "        formatted = \"\"\n",
                "        for msg in messages:\n",
                "            role = msg['role'].capitalize()\n",
                "            content = msg['content']\n",
                "            formatted += f\"{role}: {content}\\n\\n\"\n",
                "        \n",
                "        if messages[-1]['role'] != 'assistant':\n",
                "            formatted += \"Assistant: \"\n",
                "        \n",
                "        return formatted\n",
                "\n",
                "print(\"âœ… Chat í¬ë§·íŒ… í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Tokenizer ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer\n",
                "\n",
                "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "print(\"âœ… Tokenizer ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\n",
                        "<|im_start|>system\n",
                        "You are a SQL expert. Convert natural language queries to SQL.<|im_end|>\n",
                        "<|im_start|>user\n",
                        "Find all users with age greater than 30<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.78it/s, est. speed input: 103.21 toks/s, output: 33.47 toks/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸ’¡ ìƒì„±ëœ SQL:\n",
                        "SELECT * FROM users WHERE age > 30;\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Text-to-SQL ì˜ˆì œ\n",
                "messages = [\n",
                "    {\n",
                "        'role': 'system',\n",
                "        'content': 'You are a SQL expert. Convert natural language queries to SQL.'\n",
                "    },\n",
                "    {\n",
                "        'role': 'user',\n",
                "        'content': 'Find all users with age greater than 30'\n",
                "    }\n",
                "]\n",
                "\n",
                "# í”„ë¡¬í”„íŠ¸ ë³€í™˜\n",
                "prompt = format_chat_messages(messages)\n",
                "print(\"ğŸ“ ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸:\")\n",
                "print(prompt)\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# ì¶”ë¡ \n",
                "sampling_params_sql = SamplingParams(\n",
                "    max_tokens=64,\n",
                "    temperature=0.1,  # SQL ìƒì„±ì€ ë‚®ì€ temperature ì‚¬ìš©\n",
                ")\n",
                "\n",
                "outputs = llm.generate([prompt], sampling_params_sql)\n",
                "\n",
                "del llm\n",
                "print(\"\\nğŸ’¡ ìƒì„±ëœ SQL:\")\n",
                "print(outputs[0].outputs[0].text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4 LoRA Adapter ì‚¬ìš©í•˜ê¸°\n",
                "\n",
                "### 4.1 LoRAë€?\n",
                "\n",
                "**LoRA (Low-Rank Adaptation)**:\n",
                "- ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ fine-tuningí•˜ëŠ” ëŒ€ì‹ , ì‘ì€ adapterë§Œ í•™ìŠµ\n",
                "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥\n",
                "- ì—¬ëŸ¬ taskë³„ adapterë¥¼ ì‰½ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "### 4.2 vLLMì—ì„œ LoRA ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•\n",
                "\n",
                "#### ë°©ë²• 1: Runtime LoRA (ë™ì  ì ìš©)\n",
                "- ì¶”ë¡  ì‹œì ì— adapterë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ\n",
                "- ì—¬ëŸ¬ adapterë¥¼ ë¹ ë¥´ê²Œ êµì²´ ê°€ëŠ¥\n",
                "\n",
                "skeleton-2ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë“ˆì„ ì´ìš©í•˜ì‹œê±°ë‚˜ skeleton-3ì—ì„œ í•™ìŠµí•œ LoRA ëª¨ë“ˆì„ ì´ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 16:54:06 config.py:510] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
                        "WARNING 02-24 16:54:06 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
                        "WARNING 02-24 16:54:06 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
                        "INFO 02-24 16:54:06 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
                        "INFO 02-24 16:54:07 model_runner.py:1094] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n",
                        "INFO 02-24 16:54:07 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
                        "INFO 02-24 16:54:08 weight_utils.py:296] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.66it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.66it/s]\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 16:54:09 model_runner.py:1099] Loading model weights took 0.6752 GB\n",
                        "INFO 02-24 16:54:09 punica_selector.py:11] Using PunicaWrapperGPU.\n",
                        "INFO 02-24 16:54:10 worker.py:241] Memory profiling takes 1.55 seconds\n",
                        "INFO 02-24 16:54:10 worker.py:241] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.75) = 4.50GiB\n",
                        "INFO 02-24 16:54:10 worker.py:241] model weights take 0.68GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 3.16GiB.\n",
                        "INFO 02-24 16:54:10 gpu_executor.py:76] # GPU blocks: 5183, # CPU blocks: 6553\n",
                        "INFO 02-24 16:54:10 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 10.12x\n",
                        "INFO 02-24 16:54:11 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 2.16 seconds\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_21767/2306398584.py:20: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
                        "  lora_request = LoRARequest(\"my_adapter\", 1, lora_adapter_path) # my_adapterëŠ” ì„ì˜ì˜ ì´ë¦„. ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥ëŠ¥\n",
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.21s/it, est. speed input: 11.53 toks/s, output: 4.99 toks/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… LoRA adapter ì ìš© ì™„ë£Œ\n",
                        "[RequestOutput(request_id=0, prompt='<|im_start|>system\\nYou are a SQL expert. Convert natural language queries to SQL.<|im_end|>\\n<|im_start|>user\\nFind all users with age greater than 30<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[1, 9690, 198, 2683, 359, 253, 15142, 4507, 30, 29490, 1782, 1789, 18795, 288, 15142, 30, 2, 198, 1, 4093, 198, 11933, 511, 3629, 351, 1850, 2852, 670, 216, 35, 32, 2, 198, 1, 520, 9531, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='SELECT name \\nFROM users \\nWHERE age > 30', token_ids=(23428, 1462, 216, 198, 46510, 3629, 216, 198, 14779, 14298, 1850, 2986, 216, 35, 32, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1771919651.3250432, last_token_time=1771919651.3250432, first_scheduled_time=1771919651.463425, first_token_time=1771919653.877883, time_in_queue=0.1383817195892334, finished_time=1771919654.6726089, scheduler_time=0.001531099002022529, model_forward_time=None, model_execute_time=None), lora_request=LoRARequest(lora_name='my_adapter', lora_int_id=1, lora_path='./checkpoint-100', lora_local_path=None, long_lora_max_len=None, base_model_name=None), num_cached_tokens=0, multi_modal_placeholders={})]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Runtime LoRA ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "lora_adapter_path = \"./checkpoint-100\"  # ì‹¤ì œ adapter ê²½ë¡œë¡œ ë³€ê²½\n",
                "\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    from vllm.lora.request import LoRARequest\n",
                "    \n",
                "    # LoRA ì§€ì› ëª¨ë¸ ë¡œë“œ\n",
                "    llm_with_lora = LLM(\n",
                "        model=model_name,\n",
                "        enable_lora=True,\n",
                "        max_lora_rank=64,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.75,\n",
                "        trust_remote_code=True,\n",
                "        enforce_eager=True\n",
                "    )\n",
                "    \n",
                "    # LoRA request ìƒì„±\n",
                "    lora_request = LoRARequest(\"my_adapter\", 1, lora_adapter_path) # my_adapterëŠ” ì„ì˜ì˜ ì´ë¦„. ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥ëŠ¥\n",
                "    \n",
                "    # ì¶”ë¡  (LoRA adapter ì ìš©)\n",
                "    outputs = llm_with_lora.generate(\n",
                "        [prompt],\n",
                "        sampling_params,\n",
                "        lora_request=lora_request\n",
                "    )\n",
                "    \n",
                "    print(\"âœ… LoRA adapter ì ìš© ì™„ë£Œ\")\n",
                "    print(outputs)\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### ë°©ë²• 2: Merged Model (ì‚¬ì „ í†µí•©)\n",
                "- LoRA weightsë¥¼ base modelì— ë¯¸ë¦¬ merge\n",
                "- ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„\n",
                "- ë°°í¬ì— ìœ ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”„ LoRA merge ì‹œì‘...\n",
                        "âœ… Merge ì™„ë£Œ: ./merged_model\n",
                        "INFO 02-24 16:54:29 config.py:510] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
                        "INFO 02-24 16:54:29 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='./merged_model', speculative_config=None, tokenizer='./merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./merged_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
                        "INFO 02-24 16:54:29 model_runner.py:1094] Starting to load model ./merged_model...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.46s/it]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.46s/it]\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 16:54:35 model_runner.py:1099] Loading model weights took 0.6748 GB\n",
                        "INFO 02-24 16:54:36 worker.py:241] Memory profiling takes 0.88 seconds\n",
                        "INFO 02-24 16:54:36 worker.py:241] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.75) = 4.50GiB\n",
                        "INFO 02-24 16:54:36 worker.py:241] model weights take 0.67GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 3.32GiB.\n",
                        "INFO 02-24 16:54:36 gpu_executor.py:76] # GPU blocks: 5445, # CPU blocks: 6553\n",
                        "INFO 02-24 16:54:36 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 10.63x\n",
                        "INFO 02-24 16:54:37 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:18<00:00,  1.93it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 16:54:55 model_runner.py:1535] Graph capturing finished in 17 secs, took 0.20 GiB\n",
                        "INFO 02-24 16:54:55 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 19.70 seconds\n",
                        "âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "from peft import PeftModel\n",
                "\n",
                "\n",
                "\n",
                "def merge_lora_to_base(base_model_name, lora_path, output_path):\n",
                "    \"\"\"\n",
                "    LoRA adapterë¥¼ base modelì— merge\n",
                "    \"\"\"\n",
                "    print(\"ğŸ”„ LoRA merge ì‹œì‘...\")\n",
                "    \n",
                "    # 1. Base model ë¡œë“œ\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        trust_remote_code=True,\n",
                "        torch_dtype=\"auto\",\n",
                "    )\n",
                "    \n",
                "    # 2. LoRA adapter ë¡œë“œ\n",
                "    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)\n",
                "    \n",
                "    # 3. Merge\n",
                "    merged_model = model_with_lora.merge_and_unload()\n",
                "    \n",
                "    # 4. ì €ì¥\n",
                "    os.makedirs(output_path, exist_ok=True)\n",
                "    merged_model.save_pretrained(output_path, safe_serialization=True)\n",
                "    \n",
                "    # Tokenizerë„ ì €ì¥\n",
                "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    tokenizer.save_pretrained(output_path)\n",
                "\n",
                "    del base_model\n",
                "    del model_with_lora\n",
                "    del merged_model\n",
                "    \n",
                "    print(f\"âœ… Merge ì™„ë£Œ: {output_path}\")\n",
                "    return output_path\n",
                "\n",
                "# ì˜ˆì œ (adapterê°€ ìˆëŠ” ê²½ìš°)\n",
                "if os.path.exists(lora_adapter_path):\n",
                "    if locals().get('llm_with_lora'):\n",
                "        del llm_with_lora\n",
                "\n",
                "    merged_path = merge_lora_to_base(\n",
                "        base_model_name=model_name,\n",
                "        lora_path=lora_adapter_path,\n",
                "        output_path=\"./merged_model\"\n",
                "    )\n",
                "    \n",
                "    # Merged ëª¨ë¸ë¡œ ì¶”ë¡ \n",
                "    llm_merged = LLM(\n",
                "        model=merged_path,\n",
                "        tensor_parallel_size=1,\n",
                "        gpu_memory_utilization=0.75,\n",
                "        trust_remote_code=True,\n",
                "    )\n",
                "    print(\"âœ… Merged ëª¨ë¸ë¡œ ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ\")\n",
                "else:\n",
                "    print(\"âš ï¸ LoRA adapter ì—†ìŒ - ê±´ë„ˆë›°ê¸°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - Transformers vs vLLM\n",
                "\n",
                "vLLMì´ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì§ì ‘ ì¸¡ì •í•´ë´…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\n",
                        "  - í”„ë¡¬í”„íŠ¸ ìˆ˜: 5\n",
                        "  - Max Tokens: 64\n"
                    ]
                }
            ],
            "source": [
                "# ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„\n",
                "test_prompts = [\n",
                "    \"You are a SQL expert. Convert this to SQL: Find all users\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Count employees\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Show top 10 sales\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Delete inactive accounts\",\n",
                "    \"You are a SQL expert. Convert this to SQL: Update user emails\",\n",
                "]\n",
                "\n",
                "max_tokens = 64\n",
                "\n",
                "print(f\"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\")\n",
                "print(f\"  - í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(test_prompts)}\")\n",
                "print(f\"  - Max Tokens: {max_tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Transformers ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n",
                        "  ëª¨ë¸ ë¡œë”© ì¤‘...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some parameters are on the meta device because they were offloaded to the cpu.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… Transformers ì™„ë£Œ\n",
                        "  â±ï¸  ì´ ì‹œê°„: 9.99s\n",
                        "  âš¡ First Token: 143.97ms\n",
                        "  ğŸ”¥ Token/sec: 11.71\n",
                        "  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: 5307 MB\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import pynvml\n",
                "\n",
                "def get_gpu_memory_usage():\n",
                "    \"\"\"NVIDIA GPUì—ì„œ ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¸¡ì • (nvidia-smiì™€ ë™ì¼)\"\"\"\n",
                "    pynvml.nvmlInit()\n",
                "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
                "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
                "    pynvml.nvmlShutdown()\n",
                "    return info.used / 1024 / 1024  # MB ë‹¨ìœ„\n",
                "\n",
                "print(\"ğŸ”µ Transformers ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "print(\"  ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "\n",
                "# ëª¨ë¸ ë¡œë“œ\n",
                "tf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tf_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tf_model.eval()\n",
                "\n",
                "# ì¶”ë¡ \n",
                "tf_start = time.time()\n",
                "tf_total_tokens = 0\n",
                "tf_first_token_latencies = []\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    inputs = tf_tokenizer(prompt, return_tensors=\"pt\").to(tf_model.device)\n",
                "    \n",
                "    # First token ì¸¡ì •\n",
                "    ft_start = time.time()\n",
                "    with torch.no_grad():\n",
                "        outputs = tf_model(**inputs)\n",
                "    first_token_time = time.time() - ft_start\n",
                "    tf_first_token_latencies.append(first_token_time)\n",
                "    \n",
                "    # ì „ì²´ ìƒì„±\n",
                "    with torch.no_grad():\n",
                "        generated = tf_model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_tokens,\n",
                "            do_sample=False,\n",
                "        )\n",
                "    tf_total_tokens += generated.shape[1] - inputs.input_ids.shape[1]\n",
                "\n",
                "tf_time = time.time() - tf_start\n",
                "tf_peak_memory = get_gpu_memory_usage()  # NVIDIA GPU ì‹¤ì œ ë©”ëª¨ë¦¬ ì¸¡ì •\n",
                "tf_avg_first_token = sum(tf_first_token_latencies) / len(tf_first_token_latencies)\n",
                "\n",
                "print(f\"\\nâœ… Transformers ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {tf_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {tf_total_tokens / tf_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {tf_peak_memory:.0f} MB\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
                "del tf_model\n",
                "del tf_tokenizer\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 vLLM ë²¤ì¹˜ë§ˆí¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 29.83 toks/s, output: 29.83 toks/s]\n",
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.41it/s, est. speed input: 178.80 toks/s, output: 330.28 toks/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… vLLM ì™„ë£Œ\n",
                        "  â±ï¸  ì´ ì‹œê°„: 0.41s\n",
                        "  âš¡ First Token: 472.74ms\n",
                        "  ğŸ”¥ Token/sec: 327.93\n",
                        "  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: 5313 MB\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nğŸŸ¢ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘\")\n",
                "\n",
                "# Sampling íŒŒë¼ë¯¸í„°\n",
                "benchmark_params = SamplingParams(\n",
                "    max_tokens=max_tokens,\n",
                "    temperature=0.0,  # greedy decoding\n",
                ")\n",
                "\n",
                "# First token latency ì¸¡ì •\n",
                "vllm_ft_start = time.time()\n",
                "_ = llm_merged.generate([test_prompts[0]], benchmark_params)\n",
                "vllm_first_token = time.time() - vllm_ft_start\n",
                "\n",
                "# ì „ì²´ ì¶”ë¡ \n",
                "vllm_start = time.time()\n",
                "vllm_outputs = llm_merged.generate(test_prompts, benchmark_params)\n",
                "vllm_time = time.time() - vllm_start\n",
                "\n",
                "vllm_peak_memory = get_gpu_memory_usage()  # NVIDIA GPU ì‹¤ì œ ë©”ëª¨ë¦¬ ì¸¡ì •\n",
                "vllm_total_tokens = sum(len(out.outputs[0].token_ids) for out in vllm_outputs)\n",
                "\n",
                "print(f\"\\nâœ… vLLM ì™„ë£Œ\")\n",
                "print(f\"  â±ï¸  ì´ ì‹œê°„: {vllm_time:.2f}s\")\n",
                "print(f\"  âš¡ First Token: {vllm_first_token*1000:.2f}ms\")\n",
                "print(f\"  ğŸ”¥ Token/sec: {vllm_total_tokens / vllm_time:.2f}\")\n",
                "print(f\"  ğŸ’¾ í”¼í¬ ë©”ëª¨ë¦¬: {vllm_peak_memory:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 ê²°ê³¼ ë¹„êµ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\n",
                        "============================================================\n",
                        "\n",
                        "âš¡ First Token Latency:\n",
                        "  Transformers: 143.97ms\n",
                        "  vLLM:         472.74ms\n",
                        "  âš ï¸ ì°¨ì´: 228.3%\n",
                        "\n",
                        "ğŸ”¥ Token/sec:\n",
                        "  Transformers: 11.71 tokens/sec\n",
                        "  vLLM:         327.93 tokens/sec\n",
                        "  ğŸš€ vLLM í–¥ìƒ: 28.00x\n",
                        "\n",
                        "â±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\n",
                        "  Transformers: 9.99s\n",
                        "  vLLM:         0.41s\n",
                        "  ğŸš€ ì†ë„ í–¥ìƒ: 24.63x\n",
                        "\n",
                        "ğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\n",
                        "  Transformers: 5307 MB\n",
                        "  vLLM:         5313 MB\n",
                        "  ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©: 6 MB ë” ì‚¬ìš©\n",
                        "\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# First Token Latency\n",
                "print(f\"\\nâš¡ First Token Latency:\")\n",
                "print(f\"  Transformers: {tf_avg_first_token*1000:.2f}ms\")\n",
                "print(f\"  vLLM:         {vllm_first_token*1000:.2f}ms\")\n",
                "ft_improvement = ((tf_avg_first_token - vllm_first_token) / tf_avg_first_token) * 100\n",
                "print(f\"  {'ğŸš€ ê°œì„ ' if ft_improvement > 0 else 'âš ï¸ ì°¨ì´'}: {abs(ft_improvement):.1f}%\")\n",
                "\n",
                "# Token/sec\n",
                "tf_tps = tf_total_tokens / tf_time\n",
                "vllm_tps = vllm_total_tokens / vllm_time\n",
                "print(f\"\\nğŸ”¥ Token/sec:\")\n",
                "print(f\"  Transformers: {tf_tps:.2f} tokens/sec\")\n",
                "print(f\"  vLLM:         {vllm_tps:.2f} tokens/sec\")\n",
                "print(f\"  ğŸš€ vLLM í–¥ìƒ: {vllm_tps / tf_tps:.2f}x\")\n",
                "\n",
                "# ì´ ì‹œê°„\n",
                "print(f\"\\nâ±ï¸ ì´ ì¶”ë¡  ì‹œê°„:\")\n",
                "print(f\"  Transformers: {tf_time:.2f}s\")\n",
                "print(f\"  vLLM:         {vllm_time:.2f}s\")\n",
                "print(f\"  ğŸš€ ì†ë„ í–¥ìƒ: {tf_time / vllm_time:.2f}x\")\n",
                "\n",
                "# ë©”ëª¨ë¦¬\n",
                "print(f\"\\nğŸ’¾ í”¼í¬ GPU ë©”ëª¨ë¦¬:\")\n",
                "print(f\"  Transformers: {tf_peak_memory:.0f} MB\")\n",
                "print(f\"  vLLM:         {vllm_peak_memory:.0f} MB\")\n",
                "memory_diff = tf_peak_memory - vllm_peak_memory\n",
                "if memory_diff > 0:\n",
                "    print(f\"  ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆê°: {memory_diff:.0f} MB ({memory_diff/tf_peak_memory*100:.1f}%)\")\n",
                "else:\n",
                "    print(f\"  ğŸ“ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©: {abs(memory_diff):.0f} MB ë” ì‚¬ìš©\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6: ì¢…í•© ì‹¤ìŠµ\n",
                "\n",
                "### ìµœì¢… í”„ë¡œì íŠ¸: Text-to-SQL ì‹œìŠ¤í…œ êµ¬ì¶•\n",
                "\n",
                "ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì „ Text-to-SQL ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– ì¶”ë¡  ì‹œì‘...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.28it/s, est. speed input: 492.40 toks/s, output: 118.81 toks/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "âœ… ì¶”ë¡  ì™„ë£Œ! (0.76ì´ˆ)\n",
                        "ğŸ“Š ì²˜ë¦¬ëŸ‰: 5.25 prompts/sec\n",
                        "\n",
                        "[ìƒì„± ê²°ê³¼] SELECT COUNT(*) FROM pets;\n",
                        "[í† í° ìˆ˜] 9\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT d.dept_name, COUNT(*) FROM employees e\n",
                        "JOIN (SELECT dept_name, COUNT(*) FROM employees GROUP BY dept_name) d\n",
                        "GROUP BY dept_name ORDER BY COUNT(*) DESC\n",
                        "[í† í° ìˆ˜] 58\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT COUNT(*) FROM airlines\n",
                        "[í† í° ìˆ˜] 8\n",
                        "------------------------------------------------------------\n",
                        "[ìƒì„± ê²°ê³¼] SELECT * FROM departments WHERE budget > 100000\n",
                        "[í† í° ìˆ˜] 15\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "system_prompt = \"\"\"You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query.\"\"\"\n",
                "user_prompt = \"\"\"Given the <USER_QUERY>, generate the corresponding SQL command to retrieve the desired data, considering the query's syntax, semantics, and schema constraints.\n",
                "\n",
                "<USER_QUERY>\n",
                "{question}\n",
                "</USER_QUERY>\"\"\"\n",
                "\n",
                "test_questions = [\n",
                "    \"How many different types of pet are there?\",\n",
                "    \"Count employees in each department\",\n",
                "    \"How many airlines do we have?\",\n",
                "    \"List departments with budget over 100000\",\n",
                "]\n",
                "\n",
                "messages = []\n",
                "for i in range(len(test_questions)):\n",
                "    messages.append(\n",
                "        [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": system_prompt\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": user_prompt.format(question=test_questions[i])\n",
                "            }\n",
                "        ]\n",
                "    )\n",
                "\n",
                "prompts = format_chat_messages(messages)\n",
                "\n",
                "print(\"ğŸ¤– ì¶”ë¡  ì‹œì‘...\")\n",
                "start_time = time.time()\n",
                "\n",
                "outputs = llm_merged.generate(prompts, sampling_params)\n",
                "\n",
                "inference_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nâœ… ì¶”ë¡  ì™„ë£Œ! ({inference_time:.2f}ì´ˆ)\")\n",
                "print(f\"ğŸ“Š ì²˜ë¦¬ëŸ‰: {len(prompts) / inference_time:.2f} prompts/sec\\n\")\n",
                "\n",
                "# ê²°ê³¼ ì¶œë ¥\n",
                "for i, output in enumerate(outputs):\n",
                "    generated_text = output.outputs[0].text\n",
                "    token_count = len(output.outputs[0].token_ids)\n",
                "    \n",
                "    print(f\"[ìƒì„± ê²°ê³¼] {generated_text}\")\n",
                "    print(f\"[í† í° ìˆ˜] {token_count}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6.1 (ì¢…í•©) skeleton02 íŠœë‹ ì‚°ì¶œë¬¼ ìë™ ë¡œë“œ í›„ vLLM ì„œë¹™/ì¶”ë¡ \n",
                "\n",
                "ì•ì„  `skeleton02`ì—ì„œ LoRA í•™ìŠµì„ ì§„í–‰í•˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ `upstage-skeleton02-master/outputs/checkpoint-*` í˜•íƒœë¡œ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë©ë‹ˆë‹¤. (ì˜ˆ: `checkpoint-100`)\n",
                "### ì‚¬ì „ ê°œë…: Multi-LoRA / Adapter routing\n",
                "- **Multi-LoRA**: í•˜ë‚˜ì˜ ë² ì´ìŠ¤ ëª¨ë¸ ìœ„ì— íƒœìŠ¤í¬/ë„ë©”ì¸ë³„ LoRA ì–´ëŒ‘í„°ë¥¼ ì—¬ëŸ¬ ê°œ ì˜¬ë ¤ë‘ê³ , í•„ìš”í•  ë•Œ ì„ íƒí•´ì„œ ì ìš©í•˜ëŠ” ìš´ì˜ ë°©ì‹ì…ë‹ˆë‹¤.\n",
                "- **Adapter routing(ì–´ëŒ‘í„° ë¼ìš°íŒ…)**: ì¶”ë¡  **ìš”ì²­ ë‹¨ìœ„**ë¡œ ì–´ë–¤ ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. (ì˜ˆ: task, tenant, language)\n",
                "- **`LoRARequest`ì˜ ì—­í• **: vLLM ì¶”ë¡  ìš”ì²­ì— â€œì´ë²ˆ ìš”ì²­ì— ì ìš©í•  ì–´ëŒ‘í„°â€ë¥¼ ëª…ì‹œí•´, ì—¬ëŸ¬ Adapter ì¤‘ **í•˜ë‚˜ë¥¼ ì„ íƒ**í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\n",
                "\n",
                "ì¦‰, ì´ë²ˆ ì¢…í•© ì‹¤ìŠµì˜ í•µì‹¬ì€ **ìš”ì²­ë§ˆë‹¤ ì–´ëŒ‘í„°ë¥¼ ì„ íƒ/êµì²´í•˜ë©°(Text-to-SQLì— ë§ëŠ” LoRA ì ìš©)** ë™ì‘ì‹œí‚¤ëŠ” íë¦„ì„ ìµíˆëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
                "ì•„ë˜ ì½”ë“œëŠ” `skeleton04`ì—ì„œ **skeleton02ì˜ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„** LoRA adapterë¡œ ë¡œë“œí•œ ë’¤, vLLMìœ¼ë¡œ ì¶”ë¡ (=ì„œë¹™ ê´€ì ì˜ inference)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
                "\n",
                "- ê°€ì •: `upstage-skeleton02-master` í´ë”ê°€ í˜„ì¬ `upstage-skeleton04-master`ì™€ ê°™ì€ ìƒìœ„ í´ë”ì— ì¡´ì¬\n",
                "- ì°¸ê³ : skeleton02ì—ì„œ `merge_and_save=True`ë¡œ adapterë¥¼ `outputs/` ë£¨íŠ¸ì— ì €ì¥í•´ë‘” ê²½ìš°ë„ ìë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… skeleton02 LoRA adapter ë°œê²¬: /mnt/c/Users/SSAFY/Desktop/ai-special-skeleton/2. skeleton02/outputs/checkpoint-100\n",
                        "INFO 02-24 17:21:29 config.py:510] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
                        "WARNING 02-24 17:21:29 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
                        "WARNING 02-24 17:21:29 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
                        "INFO 02-24 17:21:29 llm_engine.py:234] Initializing an LLM engine (v0.6.6) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
                        "INFO 02-24 17:21:29 model_runner.py:1094] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n",
                        "INFO 02-24 17:21:30 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
                        "INFO 02-24 17:21:30 weight_utils.py:296] No model.safetensors.index.json found in remote.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.82it/s]\n",
                        "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.82it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 17:21:31 model_runner.py:1099] Loading model weights took 0.6752 GB\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO 02-24 17:21:31 punica_selector.py:11] Using PunicaWrapperGPU.\n",
                        "INFO 02-24 17:21:32 worker.py:241] Memory profiling takes 1.66 seconds\n",
                        "INFO 02-24 17:21:32 worker.py:241] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.75) = 4.50GiB\n",
                        "INFO 02-24 17:21:32 worker.py:241] model weights take 0.68GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 3.16GiB.\n",
                        "INFO 02-24 17:21:33 gpu_executor.py:76] # GPU blocks: 5183, # CPU blocks: 6553\n",
                        "INFO 02-24 17:21:33 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 10.12x\n",
                        "INFO 02-24 17:21:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 2.34 seconds\n",
                        "WARNING 02-24 17:21:33 tokenizer.py:191] No tokenizer found in /mnt/c/Users/SSAFY/Desktop/ai-special-skeleton/2. skeleton02/outputs/checkpoint-100, using base model tokenizer instead. (Exception: Tokenizer class TokenizersBackend does not exist or is not currently imported.)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_24597/624356716.py:115: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\n",
                        "  lora_request = LoRARequest(\"skeleton02\", 1, str(lora_adapter_dir))\n",
                        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.69s/it, est. speed input: 18.93 toks/s, output: 8.28 toks/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[ìƒì„± ê²°ê³¼]\n",
                        " SELECT department, COUNT(*) FROM employees GROUP BY department\n",
                        "\n",
                        "---\n",
                        "ì•„ë˜ëŠ” vLLM OpenAI ì„œë²„ë¡œ ë„ìš°ëŠ” ì˜ˆì‹œ ì»¤ë§¨ë“œì…ë‹ˆë‹¤ (í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰).\n",
                        "í™˜ê²½ì— ë”°ë¼ ì˜µì…˜ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ, ì—ëŸ¬ ì‹œ `vllm serve --help`ë¡œ í™•ì¸í•˜ì„¸ìš”.\n",
                        "vllm serve HuggingFaceTB/SmolLM2-360M-Instruct --enable-lora --max-lora-rank 64 --lora-modules skeleton02=/mnt/c/Users/SSAFY/Desktop/ai-special-skeleton/2. skeleton02/outputs/checkpoint-100\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import re\n",
                "from pathlib import Path\n",
                "\n",
                "from transformers import AutoTokenizer\n",
                "from vllm import LLM, SamplingParams\n",
                "\n",
                "\n",
                "def _find_upwards(start: Path, target_dir_name: str, max_up: int = 5) -> Path | None:\n",
                "    \"\"\"Walk up parent dirs to find a sibling directory by name.\"\"\"\n",
                "    cur = start.resolve()\n",
                "    for _ in range(max_up + 1):\n",
                "        cand = cur / target_dir_name\n",
                "        if cand.exists():\n",
                "            return cand\n",
                "        cur = cur.parent\n",
                "    return None\n",
                "\n",
                "\n",
                "def _is_lora_adapter_dir(p: Path) -> bool:\n",
                "    if not p.exists() or not p.is_dir():\n",
                "        return False\n",
                "    if not (p / \"adapter_config.json\").exists():\n",
                "        return False\n",
                "    if (p / \"adapter_model.safetensors\").exists() or (p / \"adapter_model.bin\").exists():\n",
                "        return True\n",
                "    return False\n",
                "\n",
                "\n",
                "def _find_latest_checkpoint(outputs_dir: Path) -> Path | None:\n",
                "    ckpts: list[tuple[int, Path]] = []\n",
                "    for p in outputs_dir.glob(\"checkpoint-*\"):\n",
                "        if not p.is_dir():\n",
                "            continue\n",
                "        m = re.match(r\"checkpoint-(\\d+)$\", p.name)\n",
                "        if not m:\n",
                "            continue\n",
                "        if not _is_lora_adapter_dir(p):\n",
                "            continue\n",
                "        ckpts.append((int(m.group(1)), p))\n",
                "    if not ckpts:\n",
                "        return None\n",
                "    ckpts.sort(key=lambda x: x[0])\n",
                "    return ckpts[-1][1]\n",
                "\n",
                "\n",
                "# 1) skeleton02 ì‚°ì¶œë¬¼(LoRA adapter) ìë™ íƒìƒ‰\n",
                "s2_root = _find_upwards(Path.cwd(), \"2. skeleton02\")\n",
                "if s2_root is None:\n",
                "    raise FileNotFoundError(\n",
                "        \"`upstage-skeleton02-master` í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. \"\n",
                "        \"(í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ìƒìœ„ í´ë”ë“¤ì—ì„œ íƒìƒ‰)\"\n",
                "    )\n",
                "\n",
                "outputs_dir = s2_root / \"outputs\"\n",
                "if not outputs_dir.exists():\n",
                "    raise FileNotFoundError(\n",
                "        f\"skeleton02 outputs í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {outputs_dir}\\n\"\n",
                "        \"skeleton02ì—ì„œ í•™ìŠµì„ ë¨¼ì € ì‹¤í–‰í•´ `outputs/checkpoint-*`ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\"\n",
                "    )\n",
                "\n",
                "# (a) outputs/ ë£¨íŠ¸ì— adapterë¥¼ ë”°ë¡œ ì €ì¥í•œ ê²½ìš° (merge_and_save=True)\n",
                "adapter_dir = outputs_dir if _is_lora_adapter_dir(outputs_dir) else None\n",
                "# (b) ê¸°ë³¸: outputs/checkpoint-* ì¤‘ ìµœì‹ \n",
                "ckpt_dir = _find_latest_checkpoint(outputs_dir)\n",
                "\n",
                "lora_adapter_dir = adapter_dir or ckpt_dir\n",
                "if lora_adapter_dir is None:\n",
                "    raise FileNotFoundError(\n",
                "        f\"LoRA adapterë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\\n\"\n",
                "        f\"- í™•ì¸í•œ ê²½ë¡œ: {outputs_dir} (ë£¨íŠ¸ adapter), {outputs_dir}/checkpoint-*\\n\"\n",
                "        \"skeleton02 í•™ìŠµì„ ì‹¤í–‰í•œ ë’¤, `adapter_config.json`ê³¼ `adapter_model.*`ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\"\n",
                "    )\n",
                "\n",
                "print(f\"âœ… skeleton02 LoRA adapter ë°œê²¬: {lora_adapter_dir}\")\n",
                "\n",
                "\n",
                "# 2) base ëª¨ë¸/í† í¬ë‚˜ì´ì € ì¤€ë¹„ (skeleton02 ê¸°ë³¸ê°’ê³¼ ë™ì¼)\n",
                "base_model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# format_chat_messages í•¨ìˆ˜ê°€ ì•ì—ì„œ ì •ì˜ë˜ì–´ ìˆë‹¤ë©´ ì¬ì‚¬ìš©\n",
                "if \"format_chat_messages\" not in globals():\n",
                "\n",
                "    def format_chat_messages(messages):\n",
                "        if tokenizer.chat_template:\n",
                "            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "        formatted = \"\"\n",
                "        for msg in messages:\n",
                "            role = msg[\"role\"].capitalize()\n",
                "            content = msg[\"content\"]\n",
                "            formatted += f\"{role}: {content}\\n\\n\"\n",
                "        if messages[-1][\"role\"] != \"assistant\":\n",
                "            formatted += \"Assistant: \"\n",
                "        return formatted\n",
                "\n",
                "\n",
                "# 3) vLLM Runtime LoRAë¡œ ì¶”ë¡ (=ì„œë¹™ ê´€ì ì˜ inference)\n",
                "# WSL í™˜ê²½ì—ì„œëŠ” eager ëª¨ë“œê°€ ì•ˆì •ì ì¸ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
                "llm = LLM(\n",
                "    model=base_model_name,\n",
                "    enable_lora=True,\n",
                "    max_lora_rank=64,\n",
                "    tensor_parallel_size=1,\n",
                "    gpu_memory_utilization=0.75,\n",
                "    trust_remote_code=True,\n",
                "    enforce_eager=True,\n",
                ")\n",
                "\n",
                "from vllm.lora.request import LoRARequest\n",
                "\n",
                "lora_request = LoRARequest(\"skeleton02\", 1, str(lora_adapter_dir))\n",
                "\n",
                "sampling_params = SamplingParams(max_tokens=128, temperature=0.0)\n",
                "\n",
                "system_prompt = \"You are a SQL expert. Convert natural language queries to SQL.\"\n",
                "user_q = \"Count employees in each department\"\n",
                "\n",
                "prompt = format_chat_messages(\n",
                "    [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": user_q},\n",
                "    ]\n",
                ")\n",
                "\n",
                "outputs = llm.generate([prompt], sampling_params, lora_request=lora_request)\n",
                "print(\"\\n[ìƒì„± ê²°ê³¼]\\n\", outputs[0].outputs[0].text)\n",
                "\n",
                "\n",
                "# 4) (ì„ íƒ) ì‹¤ì œ 'ì„œë¹™ ì„œë²„'ë¡œ ë„ìš°ëŠ” CLI ì˜ˆì‹œ\n",
                "print(\"\\n---\")\n",
                "print(\"ì•„ë˜ëŠ” vLLM OpenAI ì„œë²„ë¡œ ë„ìš°ëŠ” ì˜ˆì‹œ ì»¤ë§¨ë“œì…ë‹ˆë‹¤ (í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰).\")\n",
                "print(\"í™˜ê²½ì— ë”°ë¼ ì˜µì…˜ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ, ì—ëŸ¬ ì‹œ `vllm serve --help`ë¡œ í™•ì¸í•˜ì„¸ìš”.\")\n",
                "print(\n",
                "    \" \".join(\n",
                "        [\n",
                "            \"vllm serve\",\n",
                "            base_model_name,\n",
                "            \"--enable-lora\",\n",
                "            \"--max-lora-rank 64\",\n",
                "            f\"--lora-modules skeleton02={str(lora_adapter_dir)}\",\n",
                "        ]\n",
                "    )\n",
                ")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "vLLM (WSL)",
            "language": "python",
            "name": "vllm-env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
